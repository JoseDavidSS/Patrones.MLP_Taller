{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - Tarea 2\n",
    "Curso: Patrones\n",
    "\n",
    "Juan Ignacio Navarro\n",
    "\n",
    "Jose David Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el set de datos MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.8/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int32')\n",
    "\n",
    "# Normalizar las características en el rango [0, 1]\n",
    "X /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase modificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronMulticapa:\n",
    "    def __init__(self, capas, funciones_activacion, alpha=0.1, epochs=1000):\n",
    "        self.capas = capas\n",
    "        self.funciones_activacion = funciones_activacion\n",
    "        self.funcion_actual = \"s\"\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.tiempo_entrenamiento = 0.0\n",
    "        self.num_capas = len(capas)\n",
    "        self.bias = []\n",
    "        self.pesos = []\n",
    "        self.activaciones = []\n",
    "        self.deltas = []\n",
    "\n",
    "        for i in range(1, self.num_capas):\n",
    "            peso = np.random.randn(self.capas[i-1], self.capas[i])\n",
    "            self.pesos.append(peso)\n",
    "            bias = np.random.randn(self.capas[i])\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def activacion(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def activacion_derivada(self, x):\n",
    "        # Derivada de la función de activación sigmoide\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def activacion_tanh(self, x):\n",
    "        # Función de activación tanh (tangente hiperbólica)\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def activacion_derivada_tanh(self, x):\n",
    "        # Derivada de la función de activación tanh (tangente hiperbólica)\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def activacion_relu(self, x):\n",
    "        # Función de activación ReLU (Rectified Linear Unit)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activacion_derivada_relu(self, x):\n",
    "        # Derivada de la función de activación ReLU (Rectified Linear Unit)\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        self.activaciones = [X]\n",
    "        for i in range(self.num_capas - 1):\n",
    "\n",
    "            # Calcular la salida de la capa actual\n",
    "            if self.funciones_activacion[i] == \"r\": # Para ReLU\n",
    "                activacion = self.activacion_relu(np.dot(self.activaciones[i], self.pesos[i]) + self.bias[i])\n",
    "\n",
    "            elif self.funciones_activacion[i] == \"t\": # Para tanh\n",
    "                activacion = self.activacion_tanh(np.dot(self.activaciones[i], self.pesos[i]) + self.bias[i])\n",
    "\n",
    "            else: # Para sigmoide\n",
    "                activacion = self.activacion(np.dot(self.activaciones[i], self.pesos[i]) + self.bias[i])\n",
    "\n",
    "            self.activaciones.append(activacion)\n",
    "        return self.activaciones[-1]\n",
    "\n",
    "    def backpropagation(self, X, y):\n",
    "        self.deltas = []\n",
    "        salida = self.activaciones[-1]\n",
    "        error = salida - y\n",
    "        delta = error * self.activacion_derivada(salida)\n",
    "\n",
    "        self.deltas.append(delta)\n",
    "\n",
    "        # Propagar el error hacia atrás a través de la red neuronal\n",
    "        for i in reversed(range(self.num_capas - 2)):\n",
    "\n",
    "            if self.funciones_activacion[i] == \"r\":\n",
    "                delta = np.dot(self.deltas[0], self.pesos[i + 1].T) * self.activacion_derivada_relu(self.activaciones[i + 1])\n",
    "            elif self.funciones_activacion[i] == \"t\":\n",
    "                delta = np.dot(self.deltas[0], self.pesos[i + 1].T) * self.activacion_derivada_tanh(self.activaciones[i + 1])\n",
    "            else:\n",
    "                delta = np.dot(self.deltas[0], self.pesos[i + 1].T) * self.activacion_derivada(self.activaciones[i + 1])\n",
    "            \n",
    "            self.deltas.insert(0, delta)\n",
    "\n",
    "        # Actualizar pesos y bias\n",
    "        for i in range(self.num_capas - 1):\n",
    "            d_peso = np.outer(self.activaciones[i], self.deltas[i])\n",
    "            self.pesos[i] -= self.alpha * d_peso\n",
    "            self.bias[i] -= self.alpha * self.deltas[i]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tiempo_inicio = time.time()\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                self.feedforward(x)\n",
    "                self.backpropagation(x, y_true)\n",
    "        self.tiempo_entrenamiento = time.time() - tiempo_inicio\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.activaciones = [X]\n",
    "        for i in range(self.num_capas - 1):\n",
    "            # Calcular la salida de la capa actual\n",
    "            activacion = self.activacion(np.dot(self.activaciones[i], self.pesos[i]) + self.bias[i])\n",
    "            self.activaciones.append(activacion)\n",
    "        predicciones = np.argmax(self.activaciones[-1], axis=1)\n",
    "        return predicciones\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy\n",
    "    \n",
    "    def metricas(self, y_prueba, y_pred):\n",
    "        recall = recall_score(y_prueba, y_pred, average='macro')\n",
    "        precision = precision_score(y_prueba, y_pred, zero_division=1, average='macro')\n",
    "        accuracy = accuracy_score(y_prueba, y_pred)\n",
    "        f1 = f1_score(y_prueba, y_pred, average='macro')\n",
    "        return recall, precision, accuracy, f1, self.tiempo_entrenamiento\n",
    "    \n",
    "    def imprimir_metricas(self, y_prueba, y_pred):\n",
    "        recall, precision, accuracy, f1, tiempo_entrenamiento = self.metricas(y_prueba, y_pred)\n",
    "        print (\"Recall_______________________: \", recall)\n",
    "        print (\"Precision____________________: \", precision)\n",
    "        print (\"Accuracy_____________________: \", accuracy)\n",
    "        print (\"F1___________________________: \", f1)\n",
    "        print (\"Tiempo de entrenamiento______: \", tiempo_entrenamiento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra función para parametrizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myMLP (capas_ocultas, funciones_activacion, alpha, epochs):\n",
    "    funciones_activacion = [\"s\"] + funciones_activacion + [\"s\"]\n",
    "    perceptron = PerceptronMulticapa(capas_ocultas, funciones_activacion, alpha, epochs)\n",
    "    return perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de PCA como feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas únicas: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Reducción de dimensionalidad utilizando PCA\n",
    "pca = PCA(n_components=0.95)  # Mantener el 95% de la varianza explicada\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "# División en conjuntos de entrenamiento y prueba\n",
    "print(\"Etiquetas únicas:\", np.unique(y))\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X_reduced, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La dimensión de la entrada es de_____:  154\n"
     ]
    }
   ],
   "source": [
    "# Inicialización de variables\n",
    "capas_ocultas = [8, 10, 61, 30]\n",
    "funciones_activacion = [\"s\", \"r\", \"t\", \"s\"]\n",
    "alpha = 0.1\n",
    "epochs = 500\n",
    "entrada_dim = len(X_entrenamiento[0])\n",
    "print (\"La dimensión de la entrada es de_____: \", entrada_dim)\n",
    "salida_dim = 10\n",
    "capas_totales = [entrada_dim] + capas_ocultas + [salida_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación y entrenamiento de nuestro MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576/1309785483.py:23: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el perceptrón multicapa\n",
    "perceptron = myMLP(capas_totales, funciones_activacion, alpha, epochs)\n",
    "perceptron.fit(X_entrenamiento, np.eye(10)[y_entrenamiento])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones y métricas de nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones_______:  [7 7 7 ... 7 7 7]\n",
      "Recall_______________________:  0.1\n",
      "Precision____________________:  0.9106782106782105\n",
      "Accuracy_____________________:  0.10678210678210678\n",
      "F1___________________________:  0.019295958279009127\n",
      "Tiempo de entrenamiento______:  3276.690846681595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.8/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3576/1309785483.py:23: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# Verificar el tamaño de X_prueba\n",
    "if len(X_prueba) % 784 != 0:\n",
    "    # Calcular el tamaño necesario para ser divisible por 784\n",
    "    nuevo_tamano = len(X_prueba) + (784 - (len(X_prueba) % 784))\n",
    "    \n",
    "    # Ajustar el tamaño de X_prueba agregando elementos adicionales\n",
    "    X_prueba = np.concatenate((X_prueba, X_prueba[:nuevo_tamano - len(X_prueba)]))\n",
    "X_prueba_reduced = pca.transform(X_prueba.reshape(-1, 784))\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "predicciones = perceptron.predict(X_prueba_reduced)\n",
    "print(\"Predicciones_______: \", predicciones)\n",
    "\n",
    "# Imprimir métricas\n",
    "y_prueba = y_prueba.head(len(predicciones))\n",
    "perceptron.imprimir_metricas(y_prueba, predicciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
