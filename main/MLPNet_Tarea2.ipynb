{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jose/.local/lib/python3.8/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas únicas: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos de MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int32')\n",
    "\n",
    "# Normalizar las características en el rango [0, 1]\n",
    "X /= 255.0\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Etiquetas únicas:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronMulticapa:\n",
    "    def __init__(self, capas, alpha=0.1, epochs=1000):\n",
    "        self.capas = capas\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "        self.tiempo_entrenamiento = 0.0\n",
    "        self.num_capas = len(capas)\n",
    "        self.bias = []\n",
    "        self.pesos = []\n",
    "        self.activaciones = []\n",
    "        self.deltas = []\n",
    "\n",
    "        for i in range(1, self.num_capas):\n",
    "            peso = np.random.randn(self.capas[i-1], self.capas[i])\n",
    "            self.pesos.append(peso)\n",
    "            bias = np.random.randn(self.capas[i])\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def activacion(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def activacion_derivada(self, x):\n",
    "        # Derivada de la función de activación sigmoide\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        self.activaciones = [X]\n",
    "        for i in range(self.num_capas - 1):\n",
    "            # Calcular la salida de la capa actual\n",
    "            activacion = self.activacion(np.dot(self.activaciones[i], self.pesos[i]) + self.bias[i])\n",
    "            self.activaciones.append(activacion)\n",
    "        return self.activaciones[-1]\n",
    "\n",
    "    def backpropagation(self, X, y):\n",
    "        self.deltas = []\n",
    "        salida = self.activaciones[-1]\n",
    "        error = salida - y\n",
    "        delta = error * self.activacion_derivada(salida)\n",
    "        self.deltas.append(delta)\n",
    "\n",
    "        # Propagar el error hacia atrás a través de la red neuronal\n",
    "        for i in reversed(range(self.num_capas - 2)):\n",
    "            delta = np.dot(self.deltas[0], self.pesos[i + 1].T) * self.activacion_derivada(self.activaciones[i + 1])\n",
    "            self.deltas.insert(0, delta)\n",
    "\n",
    "        # Actualizar pesos y bias\n",
    "        for i in range(self.num_capas - 1):\n",
    "            d_peso = np.outer(self.activaciones[i], self.deltas[i])\n",
    "            self.pesos[i] -= self.alpha * d_peso\n",
    "            self.bias[i] -= self.alpha * self.deltas[i]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tiempo_inicio = time.time()\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(len(X)):\n",
    "                x = X[i]\n",
    "                y_true = y[i]\n",
    "                self.feedforward(x)\n",
    "                self.backpropagation(x, y_true)\n",
    "        self.tiempo_entrenamiento = time.time() - tiempo_inicio\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.activaciones = [X]\n",
    "        for i in range(self.num_capas - 1):\n",
    "            # Calcular la salida de la capa actual\n",
    "            activacion = self.activacion(np.dot(self.activaciones[i], self.pesos[i]) + self.bias[i])\n",
    "            self.activaciones.append(activacion)\n",
    "        predicciones = np.argmax(self.activaciones[-1], axis=1)\n",
    "        return predicciones\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy\n",
    "    \n",
    "    def metricas(self, y_prueba, y_pred):\n",
    "        recall = recall_score(y_prueba, y_pred, average='macro')\n",
    "        precision = precision_score(y_prueba, y_pred, zero_division=1, average='macro')\n",
    "        accuracy = accuracy_score(y_prueba, y_pred)\n",
    "        f1 = f1_score(y_prueba, y_pred, average='macro')\n",
    "        return recall, precision, accuracy, f1, self.tiempo_entrenamiento\n",
    "    \n",
    "    def imprimir_metricas(self, y_prueba, y_pred):\n",
    "        recall, precision, accuracy, f1, tiempo_entrenamiento = self.metricas(y_prueba, y_pred)\n",
    "        print (\"Recall_______________________: \", recall)\n",
    "        print (\"Precision____________________: \", precision)\n",
    "        print (\"Accuracy_____________________: \", accuracy)\n",
    "        print (\"F1___________________________: \", f1)\n",
    "        print (\"Tiempo de entrenamiento______: \", tiempo_entrenamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall_______________________:  0.8850399626474724\n",
      "Precision____________________:  0.8870327361741845\n",
      "Accuracy_____________________:  0.8864285714285715\n",
      "F1___________________________:  0.8848768388328276\n",
      "Tiempo de entrenamiento______:  23.40227174758911\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el perceptrón multicapa\n",
    "entrada_dim = X_entrenamiento.shape[1]\n",
    "perceptron = PerceptronMulticapa(capas=[entrada_dim, 10, 10], alpha=0.1, epochs=5)\n",
    "\n",
    "X_entrenamiento = X_entrenamiento.to_numpy()\n",
    "\n",
    "perceptron.fit(X_entrenamiento, np.eye(10)[y_entrenamiento])\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "predicciones = perceptron.predict(X_prueba)\n",
    "\n",
    "# Imprimir métricas\n",
    "perceptron.imprimir_metricas(y_prueba, predicciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
